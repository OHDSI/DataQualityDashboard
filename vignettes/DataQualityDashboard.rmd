---
title: "Getting Started"
author: "Clair Blacketer"
date: "`r Sys.Date()`"
header-includes:
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead{}
    - \fancyhead[CO,CE]{Getting Started}
    - \fancyfoot[CO,CE]{DataQualityDashboard Package Version `r    utils::packageVersion("DataQualityDashboard")`}
    - \fancyfoot[LE,RO]{\thepage}
    - \renewcommand{\headrulewidth}{0.4pt}
    - \renewcommand{\footrulewidth}{0.4pt}
output:
  html_document:
    number_sections: yes
    toc: yes
---

<!--
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{Getting Started}
-->

# Getting Started
***

R Installation
===============

```r
install.packages("remotes")
remotes::install_github("OHDSI/DataQualityDashboard")
```

Note
=====
To view the JSON results in the shiny application the package requires that the CDM_SOURCE table has at least one row with some details about the database. This is to ensure that some metadata is delivered along with the JSON, should it be shared. As a best practice it is recommended to always fill in this table during ETL or at least prior to running the DQD. 


Executing Data Quality Checks
==============================
  ```r

# fill out the connection details -----------------------------------------------------------------------
connectionDetails <- DatabaseConnector::createConnectionDetails(
  dbms = "", 
  user = "", 
  password = "", 
  server = "", 
  port = "", 
  extraSettings = "",
  pathToDriver = ""
)

cdmDatabaseSchema <- "yourCdmSchema" # the fully qualified database schema name of the CDM
resultsDatabaseSchema <- "yourResultsSchema" # the fully qualified database schema name of the results schema (that you can write to)
cdmSourceName <- "Your CDM Source" # a human readable name for your CDM source
cdmVersion <- "5.4" # the CDM version you are targetting. Currently supports 5.2, 5.3, and 5.4

# determine how many threads (concurrent SQL sessions) to use ----------------------------------------
numThreads <- 1 # on Redshift, 3 seems to work well

# specify if you want to execute the queries or inspect them ------------------------------------------
sqlOnly <- FALSE # set to TRUE if you just want to get the SQL scripts and not actually run the queries

# NOTES specific to sqlOnly <- TRUE option ------------------------------------------------------------
# 1. You do not need a live database connection.  Instead, connectiOnDetails only needs these parameters:
#      connectionDetails <- DatabaseConnector::createConnectionDetails(
#        dbms = "spark",
#        pathToDriver = "/"
#      )
# 2. The generated SQL also inserts the parameters & results into @resultsDatabaseSchema.@writeTableName
#    However, defaults are populated for execution_time and query_text; and the Not_Applicable rules are not applied
# 3. If you use sqlOnlyUnionCount > 1, multiple checks are unioned within a cte to speed performance.
#    On Spark, this often yields a 10x or higher throughput
# 4. Since these are fully functional queries, this can help with debugging.
# 5. Note, in order to insert metadata into output table, must set sqlOnlyIncrementalInsert <- TRUE.  Otherwise sqlOnly is backwards compatable with <= v2.2.0)

# determine how how many concurrent checks to run within a cte (only relevant for sqlOnly <- TRUE) ----
sqlOnlyUnionCount <- 1  # Number of check sqls to union in a single query.  Default is 1.  Higher numbers can improve performance (e.g. a value of 25 may be 25x faster)
# determine whether insert check results and associated metadata into output table (only relevant for sqlOnly <- TRUE) ----
sqlOnlyIncrementalInsert <- FALSE # Default is FALSE (for backwards compatability to <= v2.2.0)



# where should the results and logs go? ----------------------------------------------------------------
outputFolder <- "output"
outputFile <- "results.json"


# logging type -------------------------------------------------------------------------------------
verboseMode <- TRUE # set to FALSE if you don't want the logs to be printed to the console

# write results to table? ------------------------------------------------------------------------------
writeToTable <- TRUE # set to FALSE if you want to skip writing to a SQL table in the results schema

# specify the name of the results table (used when writeToTable <- TRUE)
writeTableName <- "dqdashboard_results"

# write results to a csv file? -----------------------------------------------------------------------
writeToCsv <- FALSE # set to FALSE if you want to skip writing to csv file
csvFile <- "" # only needed if writeToCsv is set to TRUE

# if writing to table and using Redshift, bulk loading can be initialized -------------------------------

# Sys.setenv("AWS_ACCESS_KEY_ID" = "",
#            "AWS_SECRET_ACCESS_KEY" = "",
#            "AWS_DEFAULT_REGION" = "",
#            "AWS_BUCKET_NAME" = "",
#            "AWS_OBJECT_KEY" = "",
#            "AWS_SSE_TYPE" = "AES256",
#            "USE_MPP_BULK_LOAD" = TRUE)

# which DQ check levels to run -------------------------------------------------------------------
checkLevels <- c("TABLE", "FIELD", "CONCEPT")

# which DQ checks to run? ------------------------------------
checkNames <- c() # Names can be found in inst/csv/OMOP_CDM_v5.3_Check_Descriptions.csv

# which CDM tables to exclude? ------------------------------------
tablesToExclude <- c() 

# run the job --------------------------------------------------------------------------------------
DataQualityDashboard::executeDqChecks(connectionDetails = connectionDetails, 
                              cdmDatabaseSchema = cdmDatabaseSchema, 
                              resultsDatabaseSchema = resultsDatabaseSchema,
                              cdmSourceName = cdmSourceName, 
                              numThreads = numThreads,
                              sqlOnly = sqlOnly, 
                              sqlOnlyUnionCount = sqlOnlyUnionCount,
                              sqlOnlyIncrementalInsert = sqlOnlyIncrementalInsert,
                              outputFolder = outputFolder,
                              verboseMode = verboseMode,
                              writeToTable = writeToTable,
                              writeToCsv = writeToCsv,
                              csvFile = csvFile,
                              checkLevels = checkLevels,
                              tablesToExclude = tablesToExclude,
                              checkNames = checkNames)

# inspect logs ----------------------------------------------------------------------------
ParallelLogger::launchLogViewer(logFileName = file.path(outputFolder, cdmSourceName, 
                                                        sprintf("log_DqDashboard_%s.txt", cdmSourceName)))

# (OPTIONAL) if you want to write the JSON file to the results table separately -----------------------------
jsonFilePath <- ""
DataQualityDashboard::writeJsonResultsToTable(connectionDetails = connectionDetails, 
                                              resultsDatabaseSchema = resultsDatabaseSchema, 
                                              jsonFilePath = jsonFilePath)
                                              

```

Viewing Results
================

**Launching Dashboard as Shiny App**
```r
DataQualityDashboard::viewDqDashboard(jsonFilePath)
```

**Launching on a web server**

If you have npm installed:

1. Install http-server:

```
npm install -g http-server
```

2. Name the output file *results.json* and place it in inst/shinyApps/www

3. Go to inst/shinyApps/www, then run:

```
http-server
```

View checks
===========
To see description of checks using R, execute the command below:
```
checks <- DataQualityDashboard::listDqChecks(cdmVersion = "5.3") # Put the version of the CDM you are using

```
